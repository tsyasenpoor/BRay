{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import gzip\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, precision_recall_curve\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_data(instance_path):\n",
    "    dfs = {\n",
    "        \"pathways\": None,\n",
    "        \"statistics\": None,\n",
    "        \"train_stats\": None,\n",
    "        \"test_stats\": None, \n",
    "        \"val_stats\": None\n",
    "    }\n",
    "\n",
    "    if not os.path.isdir(instance_path):\n",
    "        print(f\"Error: Directory not found at {instance_path}\")\n",
    "        return tuple(dfs.values())\n",
    "\n",
    "    for filename in os.listdir(instance_path):\n",
    "        file_path = os.path.join(instance_path, filename)\n",
    "        \n",
    "        # Skip if not a file\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if 'analysis' in filename and filename.endswith('.json.gz'):\n",
    "                print(f\"Attempting to load {filename} (analysis JSON)...\")\n",
    "                with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                dfs['pathways'] = pd.DataFrame(data) \n",
    "                print(f\"Successfully created DataFrame for {filename}\")\n",
    "            elif 'results' in filename and filename.endswith('.json.gz'):\n",
    "                with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                if isinstance(data, dict):\n",
    "                    dfs['statistics'] = pd.DataFrame([data])\n",
    "                elif isinstance(data, list): \n",
    "                    dfs['statistics'] = pd.DataFrame(data)\n",
    "                else:\n",
    "                    print(f\"Warning: statistics file {filename} has an unexpected main data type: {type(data)}. Could not convert to DataFrame.\")\n",
    "            elif 'train' in filename and filename.endswith('.csv.gz'):\n",
    "                dfs['train_stats'] = pd.read_csv(file_path, compression='gzip')\n",
    "            elif 'test' in filename and filename.endswith('.csv.gz'):\n",
    "                dfs['test_stats'] = pd.read_csv(file_path, compression='gzip')\n",
    "            elif 'val' in filename and filename.endswith('.csv.gz'):\n",
    "                dfs['val_stats'] = pd.read_csv(file_path, compression='gzip')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "    return dfs['pathways'], dfs['statistics'], dfs['train_stats'], dfs['test_stats'], dfs['val_stats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_statistics(statistics_df):\n",
    "    row_data = statistics_df.iloc[0]\n",
    "    train_metrics=row_data['train_metrics']\n",
    "    test_metrics=row_data['test_metrics']\n",
    "    val_metrics=row_data['val_metrics']\n",
    "\n",
    "    metrics_list = [\n",
    "        {'set': 'train', 'metrics': train_metrics},\n",
    "        {'set': 'validation', 'metrics': val_metrics},\n",
    "        {'set': 'test', 'metrics': test_metrics}\n",
    "    ]\n",
    "    \n",
    "    metrics_df = pd.DataFrame([\n",
    "        {'Set': 'Train', **train_metrics},\n",
    "        {'Set': 'Validation', **val_metrics},\n",
    "        {'Set': 'Test', **test_metrics}\n",
    "    ])\n",
    "\n",
    "    print(\"Metrics Summary Table:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(metrics_df.to_string(index=False, float_format='%.4f'))\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    print(f\"\\nAvailable metrics: {list(train_metrics.keys())}\")\n",
    "    \n",
    "    return metrics_list, metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_data_chunked(instance_path, chunk_size=10000):\n",
    "    dfs = {\n",
    "        \"pathways\": None,\n",
    "        \"statistics\": None,\n",
    "        \"train_stats\": None,\n",
    "        \"test_stats\": None, \n",
    "        \"val_stats\": None\n",
    "    }\n",
    "\n",
    "    if not os.path.isdir(instance_path):\n",
    "        print(f\"Error: Directory not found at {instance_path}\")\n",
    "        return tuple(dfs.values())\n",
    "\n",
    "    for filename in os.listdir(instance_path):\n",
    "        file_path = os.path.join(instance_path, filename)\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if 'analysis' in filename and filename.endswith('.json.gz'):\n",
    "                print(f\"Attempting to load {filename} (analysis JSON) in chunks...\")\n",
    "                \n",
    "                # For large JSON files, we'll read and process in chunks\n",
    "                with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                    # Try to peek at the structure first\n",
    "                    first_char = f.read(1)\n",
    "                    f.seek(0)\n",
    "                    \n",
    "                    if first_char == '[':\n",
    "                        # It's a list - process in chunks\n",
    "                        chunks = []\n",
    "                        buffer = \"\"\n",
    "                        bracket_count = 0\n",
    "                        in_string = False\n",
    "                        escape_next = False\n",
    "                        \n",
    "                        for line in f:\n",
    "                            for char in line:\n",
    "                                if escape_next:\n",
    "                                    escape_next = False\n",
    "                                    continue\n",
    "                                if char == '\\\\':\n",
    "                                    escape_next = True\n",
    "                                elif char == '\"' and not escape_next:\n",
    "                                    in_string = not in_string\n",
    "                                elif not in_string:\n",
    "                                    if char == '[':\n",
    "                                        bracket_count += 1\n",
    "                                    elif char == ']':\n",
    "                                        bracket_count -= 1\n",
    "                                \n",
    "                                buffer += char\n",
    "                                \n",
    "                                # If we have complete objects and buffer is large enough\n",
    "                                if len(buffer) > chunk_size * 1000 and bracket_count == 1 and not in_string:\n",
    "                                    if buffer.rstrip().endswith(','):\n",
    "                                        # Complete chunk\n",
    "                                        chunk_data = '[' + buffer[1:-1] + ']'\n",
    "                                        try:\n",
    "                                            chunk_df = pd.DataFrame(json.loads(chunk_data))\n",
    "                                            chunks.append(chunk_df)\n",
    "                                            buffer = \"[\"\n",
    "                                        except:\n",
    "                                            continue\n",
    "                        \n",
    "                        # Process remaining buffer\n",
    "                        if len(buffer) > 1:\n",
    "                            try:\n",
    "                                if not buffer.rstrip().endswith(']'):\n",
    "                                    buffer = buffer.rstrip().rstrip(',') + ']'\n",
    "                                chunk_df = pd.DataFrame(json.loads(buffer))\n",
    "                                chunks.append(chunk_df)\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        if chunks:\n",
    "                            dfs['pathways'] = pd.concat(chunks, ignore_index=True)\n",
    "                            print(f\"Successfully created DataFrame with {len(dfs['pathways'])} rows\")\n",
    "                    else:\n",
    "                        # It's likely a single object, try loading normally but with error handling\n",
    "                        try:\n",
    "                            data = json.load(f)\n",
    "                            dfs['pathways'] = pd.DataFrame(data)\n",
    "                        except MemoryError:\n",
    "                            print(f\"File {filename} too large to load into memory, skipping...\")\n",
    "                            continue\n",
    "                        \n",
    "            elif 'results' in filename and filename.endswith('.json.gz'):\n",
    "                with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                if isinstance(data, dict):\n",
    "                    dfs['statistics'] = pd.DataFrame([data])\n",
    "                elif isinstance(data, list): \n",
    "                    dfs['statistics'] = pd.DataFrame(data)\n",
    "                else:\n",
    "                    print(f\"Warning: statistics file {filename} has an unexpected main data type: {type(data)}. Could not convert to DataFrame.\")\n",
    "            elif 'train' in filename and filename.endswith('.csv.gz'):\n",
    "                dfs['train_stats'] = pd.read_csv(file_path, compression='gzip')\n",
    "            elif 'test' in filename and filename.endswith('.csv.gz'):\n",
    "                dfs['test_stats'] = pd.read_csv(file_path, compression='gzip')\n",
    "            elif 'val' in filename and filename.endswith('.csv.gz'):\n",
    "                dfs['val_stats'] = pd.read_csv(file_path, compression='gzip')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {filename}: {e}\")\n",
    "    return dfs['pathways'], dfs['statistics'], dfs['train_stats'], dfs['test_stats'], dfs['val_stats']\n",
    "\n",
    "# pathinit_path=\"/labs/Aguiar/SSPA_BRAY/BRay/Results/pathway_initiated\"\n",
    "# runs_accuracy = [] \n",
    "\n",
    "# # Loop through each folder in the combined_path directory\n",
    "# for folder in os.listdir(pathinit_path):\n",
    "#     folder_path = os.path.join(pathinit_path, folder)\n",
    "#     if os.path.isdir(folder_path):\n",
    "#         # Process files within the folder using the chunked function\n",
    "#         _, statistics_df, _, _, _ = load_experiment_data_chunked(folder_path)\n",
    "#         if statistics_df is not None:\n",
    "#             # Analyze the statistics dataframe to get metric summaries\n",
    "#             _, metrics_df = analyze_statistics(statistics_df)\n",
    "#             # Check if accuracy is one of the available metrics\n",
    "#             if 'accuracy' in metrics_df.columns:\n",
    "#                 for _, row in metrics_df.iterrows():\n",
    "#                     runs_accuracy.append({\n",
    "#                         'Run': folder,\n",
    "#                         'Set': row['Set'],\n",
    "#                         'Accuracy': row['accuracy']\n",
    "#                     })\n",
    "#             else:\n",
    "#                 print(f\"Run {folder} does not report an accuracy metric.\")\n",
    "#         else:\n",
    "#             print(f\"Run {folder} did not have statistics data available.\")\n",
    "\n",
    "# # Create a combined dataframe with the accuracy for all runs\n",
    "# df_accuracy = pd.DataFrame(runs_accuracy)\n",
    "# print(\"\\nAggregated Accuracy Table:\")\n",
    "# print(df_accuracy.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sort df_accuracy by validation set accuracy in descending order\n",
    "# df_accuracy_sorted = df_accuracy[df_accuracy['Set'] == 'Validation'].sort_values('Accuracy', ascending=False)\n",
    "\n",
    "# print(\"Validation Accuracy Rankings (Highest to Lowest):\")\n",
    "# print(\"=\" * 60)\n",
    "# print(df_accuracy_sorted.to_string(index=False))\n",
    "\n",
    "# # Find the run with highest validation accuracy\n",
    "# best_run = df_accuracy_sorted.iloc[0]['Run']\n",
    "# best_val_accuracy = df_accuracy_sorted.iloc[0]['Accuracy']\n",
    "\n",
    "# print(f\"\\nBest performing run: {best_run}\")\n",
    "# print(f\"Highest validation accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "# # Show all metrics for the best run\n",
    "# best_run_metrics = df_accuracy[df_accuracy['Run'] == best_run]\n",
    "# print(f\"\\nComplete metrics for best run ({best_run}):\")\n",
    "# print(best_run_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_exp=\"/labs/Aguiar/SSPA_BRAY/BRay/Results/pathway_initiated/20250612_122135\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "best_pathways, best_statistics, best_train_stats, best_test_stats, best_val_stats = load_experiment_data(best_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a memory-efficient approach to examine the file structure\n",
    "# analysis_file = \"/labs/Aguiar/SSPA_BRAY/BRay/Results/pathway_initiated/20250522_150830/ajm_cyto_cyto_initialized_pw1272_gene_program_analysis.json.gz\"\n",
    "\n",
    "# # First, let's check the file size\n",
    "# file_size = os.path.getsize(analysis_file)\n",
    "# print(f\"File size: {file_size / (1024**3):.2f} GB\")\n",
    "\n",
    "# # Read just the beginning to understand the structure\n",
    "# with gzip.open(analysis_file, 'rt', encoding='utf-8') as f:\n",
    "#     # Read first few characters to see if it's a list or dict\n",
    "#     first_chars = f.read(100)\n",
    "#     print(f\"First 100 characters: {first_chars}\")\n",
    "    \n",
    "#     # Reset file pointer\n",
    "#     f.seek(0)\n",
    "    \n",
    "#     # Try to read line by line to find structure\n",
    "#     line_count = 0\n",
    "#     for line in f:\n",
    "#         if line_count == 0:\n",
    "#             print(f\"First line: {line[:200]}...\")\n",
    "#         line_count += 1\n",
    "#         if line_count >= 5:  # Only read first few lines\n",
    "#             break\n",
    "    \n",
    "#     print(f\"Read {line_count} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# The file is very large (0.35 GB) and has a nested structure\n",
    "# Let's try a more targeted approach to see if we can extract specific parts\n",
    "\n",
    "# First, let's see what the structure looks like in more detail\n",
    "with gzip.open(analysis_file, 'rt', encoding='utf-8') as f:\n",
    "    # Read a larger sample to understand the structure\n",
    "    sample = f.read(1000)\n",
    "    print(\"Structure sample:\")\n",
    "    print(sample)\n",
    "    \n",
    "    # Check if this is a dictionary with gene programs\n",
    "    f.seek(0)\n",
    "    try:\n",
    "        # Try to load just the first level to see the keys\n",
    "        \n",
    "        # Read line by line until we find the main structure\n",
    "        content = \"\"\n",
    "        brace_count = 0\n",
    "        for line in f:\n",
    "            content += line\n",
    "            brace_count += line.count('{') - line.count('}')\n",
    "            if len(content) > 10000 and brace_count == 0:  # Stop at first complete object\n",
    "                break\n",
    "        \n",
    "        # Try to parse what we have so far\n",
    "        if content.strip().endswith(','):\n",
    "            content = content.strip()[:-1]\n",
    "        \n",
    "        partial_data = json.loads(content)\n",
    "        print(f\"\\nTop-level keys: {list(partial_data.keys())}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse as JSON: {e}\")\n",
    "        print(\"This file is likely too large and complex to convert to a single DataFrame\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bray_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

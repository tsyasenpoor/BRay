{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course. Let's break down the model you've described and derive the complete conditional distributions needed for a Gibbs sampler.\n",
    "\n",
    "This is a very interesting model that combines a Gamma-Poisson factorization (often used in topic modeling or recommender systems) with a Bayesian logistic regression model, linked by the latent factor `θ_i`.\n",
    "\n",
    "### 1. Model Specification\n",
    "\n",
    "First, let's formalize the model based on your description and the provided diagram.\n",
    "\n",
    "**Data:**\n",
    "*   `x_ij`: Count data for user `i` and item `j` (for `i=1..n`, `j=1..p`).\n",
    "*   `y_ik`: Binary outcome for user `i` and task `k` (for `i=1..n`, `k=1..κ`).\n",
    "*   `x_i^aux`: Auxiliary feature vector for user `i`.\n",
    "\n",
    "**Latent Variables & Priors:**\n",
    "*   **Poisson Rate Factors:**\n",
    "    *   `θ_i`: User-specific factor. `θ_i ~ Gamma(α_θ, ξ_i)`\n",
    "    *   `β_j`: Item-specific factor. `β_j ~ Gamma(α_β, η_j)`\n",
    "*   **Hyperpriors:**\n",
    "    *   `ξ_i ~ Gamma(a_ξ, b_ξ)`\n",
    "    *   `η_j ~ Gamma(a_η, b_η)`\n",
    "*   **Logistic Regression Coefficients:**\n",
    "    *   `γ_k`: Coefficients for auxiliary features. `γ_k ~ N(μ_γ, Σ_γ)`\n",
    "    *   `ν_k`: Coefficients for latent factors `θ_i`. This has a **Spike-and-Slab** prior. Let's define this as:\n",
    "        *   `s_k ~ Bernoulli(π_ν)` (The \"spike\" selector)\n",
    "        *   `w_k ~ N(μ_w, σ_w^2)` (The \"slab\" value)\n",
    "        *   `ν_k = s_k * w_k`\n",
    "\n",
    "**Likelihoods:**\n",
    "1.  **Count Data:** `x_ij | θ_i, β_j ~ Poisson(θ_i * β_j)`\n",
    "    *   *Note: You mentioned `θ^T β`. Given the indices, `θ_i * β_j` is the standard interpretation for this type of model, where both are scalars.*\n",
    "2.  **Binary Outcome:** `y_ik | θ_i, γ_k, ν_k, x_i^aux ~ Bernoulli(σ(z_{ik}))`\n",
    "    *   where `z_{ik} = γ_k^T x_i^aux + ν_k θ_i`\n",
    "    *   and `σ(z) = 1 / (1 + exp(-z))` is the sigmoid function.\n",
    "\n",
    "### 2. Gibbs Sampling: Deriving the Complete Conditionals\n",
    "\n",
    "The core idea of Gibbs sampling is to iteratively sample each latent variable from its distribution conditioned on the current values of all other variables and the observed data. This conditional distribution is proportional to all terms in the full joint probability that involve the variable in question.\n",
    "\n",
    "---\n",
    "\n",
    "#### Conditional for `β_j`\n",
    "\n",
    "*   **Depends on:** Its parent `η_j` and its children `x_ij` (for all `i=1..n`).\n",
    "*   **Derivation:**\n",
    "    `p(β_j | ...)` ∝ `p(β_j | η_j) * ∏_i p(x_ij | θ_i, β_j)`\n",
    "    `∝ [β_j^(α_β-1) * exp(-η_j β_j)] * ∏_i [(θ_i β_j)^(x_ij) * exp(-θ_i β_j)]`\n",
    "    `∝ β_j^(α_β-1) * exp(-η_j β_j) * (β_j)^(∑_i x_ij) * exp(-β_j ∑_i θ_i)`\n",
    "    `∝ β_j^((α_β + ∑_i x_ij) - 1) * exp(-(η_j + ∑_i θ_i) β_j)`\n",
    "    This is the kernel of a Gamma distribution.\n",
    "*   **Result:**\n",
    "    **`β_j | ... ~ Gamma(shape = α_β + ∑_{i=1}^n x_ij, rate = η_j + ∑_{i=1}^n θ_i)`**\n",
    "\n",
    "---\n",
    "\n",
    "#### Conditional for `η_j`\n",
    "\n",
    "*   **Depends on:** Its child `β_j` and its own hyperparameters `a_η, b_η`.\n",
    "*   **Derivation:**\n",
    "    `p(η_j | ...)` ∝ `p(η_j) * p(β_j | η_j)`\n",
    "    `∝ [η_j^(a_η-1) * exp(-b_η η_j)] * [η_j^(α_β) * exp(-η_j β_j)]`\n",
    "    `∝ η_j^((a_η + α_β) - 1) * exp(-(b_η + β_j) η_j)`\n",
    "    This is also a Gamma kernel.\n",
    "*   **Result:**\n",
    "    **`η_j | ... ~ Gamma(shape = a_η + α_β, rate = b_η + β_j)`**\n",
    "\n",
    "---\n",
    "\n",
    "#### Conditional for `ξ_i`\n",
    "\n",
    "*   This is perfectly symmetric to the derivation for `η_j`.\n",
    "*   **Depends on:** Its child `θ_i` and its own hyperparameters `a_ξ, b_ξ`.\n",
    "*   **Result:**\n",
    "    **`ξ_i | ... ~ Gamma(shape = a_ξ + α_θ, rate = b_ξ + θ_i)`**\n",
    "\n",
    "---\n",
    "\n",
    "#### Conditional for `θ_i`\n",
    "\n",
    "*   This is the most complex variable as it links the two likelihoods.\n",
    "*   **Depends on:** Its parent `ξ_i`, its children `x_ij` (for all `j`), and its children `y_ik` (for all `k`), plus the co-parents of `y_ik` (`γ_k`, `ν_k`).\n",
    "*   **Derivation:**\n",
    "    `p(θ_i | ...)` ∝ `p(θ_i | ξ_i) * [∏_j p(x_ij | θ_i, β_j)] * [∏_k p(y_ik | θ_i, γ_k, ν_k)]`\n",
    "    `∝ [θ_i^(α_θ-1) exp(-ξ_i θ_i)] * [∏_j (θ_i β_j)^(x_ij) exp(-θ_i β_j)] * [∏_k σ(γ_k^T x_i^aux + ν_k θ_i)^(y_ik) * (1-σ(...))^(1-y_ik)]`\n",
    "    `∝ θ_i^((α_θ + ∑_j x_ij) - 1) * exp(-(ξ_i + ∑_j β_j) θ_i) * ∏_k p(y_ik | θ_i, ...)`\n",
    "*   **Result:** This distribution is **not a standard form** because `θ_i` is inside the sigmoid function in the logistic regression term. You cannot sample from it directly.\n",
    "    **`p(θ_i | ...) ∝ θ_i^((α_θ + ∑_j x_ij) - 1) * exp(-(ξ_i + ∑_j β_j) θ_i) * ∏_{k=1}^κ [σ(γ_k^T x_i^aux + ν_k θ_i)]^(y_ik) [1 - σ(γ_k^T x_i^aux + ν_k θ_i)]^(1-y_ik)`**\n",
    "    *   **Action:** You must use a sampling method like a **Metropolis-Hastings** step or a **Slice Sampler** to draw a new value for `θ_i`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Conditional for `γ_k`\n",
    "\n",
    "*   **Depends on:** Its prior `(μ_γ, Σ_γ)` and its children `y_ik` (for all `i`).\n",
    "*   **Derivation:**\n",
    "    `p(γ_k | ...)` ∝ `p(γ_k) * ∏_i p(y_ik | γ_k, ...)`\n",
    "    `∝ N(γ_k | μ_γ, Σ_γ) * ∏_i σ(γ_k^T x_i^aux + ν_k θ_i)^(y_ik) * (1-σ(...))^(1-y_ik)`\n",
    "*   **Result:** This is the standard posterior for Bayesian logistic regression. It is also **not a standard form**.\n",
    "    **`p(γ_k | ...) ∝ exp(-1/2 (γ_k - μ_γ)^T Σ_γ⁻¹ (γ_k - μ_γ)) * ∏_{i=1}^n [σ(z_{ik})^(y_ik) * (1-σ(z_{ik}))^(1-y_ik)]`**\n",
    "    *   **Action:** Use a **Metropolis-Hastings** step (e.g., with a multivariate Normal proposal distribution) or see the note on Polya-Gamma augmentation below.\n",
    "\n",
    "---\n",
    "\n",
    "#### Conditionals for Spike-and-Slab `ν_k` (sampling `s_k` and `w_k`)\n",
    "\n",
    "We sample the binary indicator `s_k` and the slab value `w_k` sequentially.\n",
    "\n",
    "**1. Conditional for `s_k` (the \"spike\")**\n",
    "*   This is a Bernoulli variable. We need to find the probability of it being 1 vs 0.\n",
    "*   `p(s_k=1 | ...)` ∝ `p(s_k=1) * p(Y | s_k=1, ...)` = `π_ν * ∏_i p(y_ik | ν_k = w_k, ...)`\n",
    "*   `p(s_k=0 | ...)` ∝ `p(s_k=0) * p(Y | s_k=0, ...)` = `(1-π_ν) * ∏_i p(y_ik | ν_k = 0, ...)`\n",
    "*   **Derivation:**\n",
    "    Let `L_1 = π_ν * ∏_i σ(γ_k^T x_i^aux + w_k θ_i)^(y_ik) * (1-σ(...))^(1-y_ik)`\n",
    "    Let `L_0 = (1-π_ν) * ∏_i σ(γ_k^T x_i^aux)^(y_ik) * (1-σ(...))^(1-y_ik)`\n",
    "    The probability of `s_k` being 1 is `P(s_k=1) = L_1 / (L_1 + L_0)`.\n",
    "*   **Result:**\n",
    "    **`s_k | ... ~ Bernoulli(p = L_1 / (L_1 + L_0))`**\n",
    "\n",
    "**2. Conditional for `w_k` (the \"slab\")**\n",
    "*   This is conditioned on the value of `s_k` we just sampled.\n",
    "*   **If `s_k = 0`:** `w_k` does not appear in the likelihood. We just sample it from its prior.\n",
    "    *   **Result:** **`w_k | s_k=0, ... ~ N(μ_w, σ_w^2)`**\n",
    "*   **If `s_k = 1`:** `ν_k = w_k`, so `w_k` is in the likelihood.\n",
    "    *   `p(w_k | s_k=1, ...)` ∝ `p(w_k) * ∏_i p(y_ik | ν_k=w_k, ...)`\n",
    "    *   `∝ N(w_k | μ_w, σ_w^2) * ∏_i σ(γ_k^T x_i^aux + w_k θ_i)^(y_ik) * (1-σ(...))^(1-y_ik)`\n",
    "    *   **Result:** Again, this is **not a standard form**.\n",
    "        **`p(w_k | s_k=1, ...) ∝ exp(-(w_k - μ_w)² / (2σ_w²)) * ∏_{i=1}^n [σ(z_{ik})^(y_ik) * (1-σ(z_{ik}))^(1-y_ik)]`** where `z_{ik}` now includes `w_k`.\n",
    "    *   **Action:** Use a **Metropolis-Hastings** step to sample `w_k`.\n",
    "\n",
    "### 3. A More Advanced (and Efficient) Alternative: Polya-Gamma Augmentation\n",
    "\n",
    "The non-conjugacy in the logistic regression parts (`θ_i`, `γ_k`, `w_k`) makes standard Gibbs sampling slow and difficult to tune. A very powerful technique to solve this is **Polya-Gamma data augmentation**.\n",
    "\n",
    "The key idea is that any logistic likelihood can be written as a mixture of Gaussians with respect to a Polya-Gamma distribution.\n",
    "`σ(z)^a * (1-σ(z))^b ∝ exp(cz) * ∫ exp(-ωz²/2) * p(ω|a+b, 0) dω` where `c= (a-b)/2`.\n",
    "\n",
    "By introducing an auxiliary Polya-Gamma variable `ω_{ik}` for each observation `y_{ik}`, the posteriors for `θ_i`, `γ_k`, and `w_k` become **conditionally Gaussian**, which allows for direct (Gibbs) sampling instead of Metropolis-Hastings.\n",
    "\n",
    "If you implement this:\n",
    "1.  **Augment:** For each `i, k`, you sample `ω_{ik} ~ PG(1, z_{ik})`.\n",
    "2.  **Sample `γ_k`:** The conditional for `γ_k` becomes a Multivariate Normal.\n",
    "3.  **Sample `ν_k` (or `w_k`):** The conditional for `w_k` (when `s_k=1`) becomes a Normal distribution.\n",
    "4.  **Sample `θ_i`:** The conditional for `θ_i` becomes a product of a Gamma-like term and a Gaussian term. This is still not a standard form, but it's much easier to handle (e.g., via a custom slice sampler or a very efficient Metropolis step) than the original form.\n",
    "\n",
    "This is a more advanced implementation but is the standard for high-performance inference in such models. If you are serious about implementing this, I highly recommend reading the original paper by Polson, Scott, and Windle (2013), \"Bayesian inference for logistic models using Polya-Gamma latent variables.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import Generator, default_rng\n",
    "from scipy.special import expit, gammaln, logsumexp\n",
    "from scipy.stats import gamma as gamma_dist\n",
    "from scipy.stats import norm as norm_dist\n",
    "\n",
    "class BayesianHybridModelSampler:\n",
    "    \"\"\"\n",
    "    Gibbs sampler for a hybrid Poisson Factorization and Logistic Regression model.\n",
    "\n",
    "    This model links latent factors from count data (Poisson factorization) to a\n",
    "    binary outcome (Logistic Regression).\n",
    "\n",
    "    The factorization part `x_ij ~ Poisson(theta_i @ beta_j.T)` is handled via\n",
    "    data augmentation, introducing latent counts `z_ijl` to restore conjugacy.\n",
    "\n",
    "    The logistic regression part `y_ik ~ Bernoulli(sigmoid(..))` involves non-conjugate\n",
    "    posteriors, which are handled with Metropolis-Hastings steps.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        X_aux: np.ndarray,\n",
    "        n_latent_dims: int,\n",
    "        *,\n",
    "        # Gamma shape hyperparameters\n",
    "        alpha_theta: float = 0.3,\n",
    "        alpha_beta: float = 0.3,\n",
    "        alpha_xi: float = 0.3,\n",
    "        alpha_eta: float = 0.3,\n",
    "        # Gamma rate hyperparameters\n",
    "        lambda_xi: float = 0.3,\n",
    "        lambda_eta: float = 0.3,\n",
    "        # Spike-and-Slab hyperparameters for nu (upsilon)\n",
    "        pi_nu: float = 0.5,           # Prior probability of inclusion\n",
    "        sigma_slab_sq: float = 1.0,   # Variance of the \"slab\"\n",
    "        # Prior variance for gamma\n",
    "        sigma_gamma_sq: float = 1.0,\n",
    "        seed: int | None = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the sampler with data and hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            X: Count data matrix of shape (n_users, p_items).\n",
    "            Y: Binary outcome matrix of shape (n_users, k_tasks).\n",
    "            X_aux: Auxiliary features matrix of shape (n_users, q_features).\n",
    "            n_latent_dims: The number of latent dimensions, d.\n",
    "            alpha_*: Shape parameters for Gamma priors.\n",
    "            lambda_*: Rate parameters for Gamma hyperpriors.\n",
    "            pi_nu: Prior probability for the slab component in the spike-and-slab prior.\n",
    "            sigma_slab_sq: Variance for the slab component.\n",
    "            sigma_gamma_sq: Prior variance for the gamma coefficients.\n",
    "            seed: Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.rng: Generator = default_rng(seed)\n",
    "\n",
    "        # -- Data Dimensions --\n",
    "        self.X = np.asarray(X, dtype=np.int32)\n",
    "        self.Y = np.asarray(Y, dtype=np.int32)\n",
    "        self.X_aux = np.asarray(X_aux)\n",
    "\n",
    "        self.n, self.p = self.X.shape\n",
    "        self.k = self.Y.shape[1]\n",
    "        self.d = n_latent_dims\n",
    "        self.q = self.X_aux.shape[1]\n",
    "\n",
    "        # -- Hyperparameters --\n",
    "        self.alpha_theta = alpha_theta\n",
    "        self.alpha_beta = alpha_beta\n",
    "        self.alpha_xi = alpha_xi\n",
    "        self.alpha_eta = alpha_eta\n",
    "        self.lambda_xi = lambda_xi\n",
    "        self.lambda_eta = lambda_eta\n",
    "        self.pi_nu = pi_nu\n",
    "        self.sigma_slab_sq = sigma_slab_sq\n",
    "        self.sigma_gamma_sq = sigma_gamma_sq\n",
    "\n",
    "        # -- Initialize Parameters --\n",
    "        self._init_params()\n",
    "\n",
    "    def _init_params(self) -> None:\n",
    "        \"\"\"Initialise latent variables and parameters.\"\"\"\n",
    "        self.theta = self.rng.gamma(1.0, 1.0, size=(self.n, self.d))\n",
    "        self.beta = self.rng.gamma(1.0, 1.0, size=(self.p, self.d))\n",
    "        self.xi = self.rng.gamma(self.alpha_xi, scale=1.0/self.lambda_xi, size=self.n)\n",
    "        self.eta = self.rng.gamma(self.alpha_eta, scale=1.0/self.lambda_eta, size=self.p)\n",
    "\n",
    "        # Latent counts for Poisson data augmentation\n",
    "        self.z = np.zeros((self.n, self.p, self.d), dtype=np.int32)\n",
    "        # We need an initial update to populate z based on initial theta and beta\n",
    "        self._update_latent_counts_z()\n",
    "\n",
    "        # Logistic Regression Parameters\n",
    "        self.gamma = self.rng.normal(0.0, np.sqrt(self.sigma_gamma_sq), size=(self.k, self.q))\n",
    "        \n",
    "        # Spike-and-Slab parameters (nu is upsilon)\n",
    "        # s_k is the binary \"spike\" selector, w_k is the \"slab\" value\n",
    "        self.s_nu = self.rng.binomial(1, self.pi_nu, size=(self.k, self.d))\n",
    "        self.w_nu = self.rng.normal(0.0, np.sqrt(self.sigma_slab_sq), size=(self.k, self.d))\n",
    "        self.nu = self.s_nu * self.w_nu\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # --- Poisson Factorization Updates (with Data Augmentation) ---\n",
    "    # ------------------------------------------------------------------\n",
    "    def _update_latent_counts_z(self) -> None:\n",
    "        \"\"\"Sample the latent counts z_ijl using a Multinomial distribution.\"\"\"\n",
    "        # Calculate rates for each latent dimension component\n",
    "        rates = self.theta[:, np.newaxis, :] * self.beta[np.newaxis, :, :] # Shape (n, p, d)\n",
    "        total_rates = np.sum(rates, axis=2)\n",
    "        \n",
    "        # Avoid division by zero for cases where total_rate is 0\n",
    "        total_rates[total_rates == 0] = 1.0\n",
    "        \n",
    "        # Calculate multinomial probabilities\n",
    "        probs = rates / total_rates[..., np.newaxis]\n",
    "\n",
    "        # Sample from Multinomial for each (i, j)\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.p):\n",
    "                if self.X[i, j] > 0:\n",
    "                    self.z[i, j, :] = self.rng.multinomial(n=self.X[i, j], pvals=probs[i, j, :])\n",
    "                else:\n",
    "                    self.z[i, j, :] = 0\n",
    "\n",
    "    def _update_beta(self) -> None:\n",
    "        \"\"\"Update beta_jl using its conjugate Gamma posterior.\"\"\"\n",
    "        # Sum over the 'n' dimension of the latent counts z\n",
    "        z_sum_over_i = np.sum(self.z, axis=0)  # Shape (p, d)\n",
    "        \n",
    "        shape = self.alpha_beta + z_sum_over_i\n",
    "        rate = self.eta[:, np.newaxis] + np.sum(self.theta, axis=0)\n",
    "        \n",
    "        # Correct usage of scipy.stats.gamma with rate parameter\n",
    "        self.beta = self.rng.gamma(shape, scale=1.0 / rate)\n",
    "\n",
    "    def _update_eta(self) -> None:\n",
    "        \"\"\"Update eta_j (hyper-priors for beta).\"\"\"\n",
    "        shape = self.alpha_eta + self.d * self.alpha_beta\n",
    "        rate = self.lambda_eta + np.sum(self.beta, axis=1)\n",
    "        self.eta = self.rng.gamma(shape, scale=1.0 / rate)\n",
    "\n",
    "    def _update_xi(self) -> None:\n",
    "        \"\"\"Update xi_i (hyper-priors for theta).\"\"\"\n",
    "        shape = self.alpha_xi + self.d * self.alpha_theta\n",
    "        rate = self.lambda_xi + np.sum(self.theta, axis=1)\n",
    "        self.xi = self.rng.gamma(shape, scale=1.0 / rate)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # --- Non-Conjugate Updates (Metropolis-Hastings) ---\n",
    "    # ------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _log_likelihood_bernoulli(y: np.ndarray, logits: np.ndarray) -> float:\n",
    "        \"\"\"Numerically stable log-likelihood for a Bernoulli model.\"\"\"\n",
    "        return np.sum(y * logits - np.log1p(np.exp(logits)))\n",
    "\n",
    "    def _log_posterior_theta_i(self, i: int, theta_i: np.ndarray) -> float:\n",
    "        \"\"\"Calculate log posterior for a single theta_i vector.\"\"\"\n",
    "        # Log-prior from Gamma distributions\n",
    "        log_prior = np.sum(gamma_dist.logpdf(theta_i, a=self.alpha_theta, scale=1.0/self.xi[i]))\n",
    "        \n",
    "        # Log-likelihood from Poisson factorization part (using latent z)\n",
    "        log_lik_poisson = np.sum(\n",
    "            self.z[i, :, :] * np.log(self.beta) - self.beta * theta_i[:, np.newaxis].T\n",
    "        )\n",
    "        \n",
    "        # Log-likelihood from Logistic Regression part\n",
    "        logits = self.X_aux[i] @ self.gamma.T + theta_i @ self.nu.T # Shape (k,)\n",
    "        log_lik_logistic = self._log_likelihood_bernoulli(self.Y[i, :], logits)\n",
    "        \n",
    "        return log_prior + log_lik_poisson + log_lik_logistic\n",
    "\n",
    "    def _update_theta(self, step_size: float = 0.05) -> None:\n",
    "        \"\"\"Update theta using Metropolis-Hastings due to non-conjugacy.\"\"\"\n",
    "        for i in range(self.n):\n",
    "            current_theta_i = self.theta[i]\n",
    "            # Use a log-normal proposal to keep theta positive\n",
    "            proposal_theta_i = self.rng.lognormal(np.log(current_theta_i), step_size)\n",
    "            \n",
    "            log_p_curr = self._log_posterior_theta_i(i, current_theta_i)\n",
    "            log_p_prop = self._log_posterior_theta_i(i, proposal_theta_i)\n",
    "            \n",
    "            # Jacobian correction for log-normal proposal\n",
    "            log_p_curr += np.sum(np.log(current_theta_i))\n",
    "            log_p_prop += np.sum(np.log(proposal_theta_i))\n",
    "            \n",
    "            if self.rng.random() < np.exp(log_p_prop - log_p_curr):\n",
    "                self.theta[i] = proposal_theta_i\n",
    "                \n",
    "    def _log_posterior_gamma_k(self, k: int, gamma_k: np.ndarray) -> float:\n",
    "        \"\"\"Calculate log posterior for a single gamma_k vector.\"\"\"\n",
    "        log_prior = norm_dist.logpdf(gamma_k, 0, np.sqrt(self.sigma_gamma_sq)).sum()\n",
    "        logits = self.X_aux @ gamma_k + self.theta @ self.nu[k]\n",
    "        log_lik = self._log_likelihood_bernoulli(self.Y[:, k], logits)\n",
    "        return log_prior + log_lik\n",
    "\n",
    "    def _update_gamma(self, step_size: float = 0.05) -> None:\n",
    "        \"\"\"Update gamma using Metropolis-Hastings.\"\"\"\n",
    "        for k in range(self.k):\n",
    "            current_gamma_k = self.gamma[k]\n",
    "            proposal_gamma_k = self.rng.normal(current_gamma_k, step_size)\n",
    "            \n",
    "            log_p_curr = self._log_posterior_gamma_k(k, current_gamma_k)\n",
    "            log_p_prop = self._log_posterior_gamma_k(k, proposal_gamma_k)\n",
    "            \n",
    "            if self.rng.random() < np.exp(log_p_prop - log_p_curr):\n",
    "                self.gamma[k] = proposal_gamma_k\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # --- Spike-and-Slab Updates for nu (upsilon) ---\n",
    "    # ------------------------------------------------------------------\n",
    "    def _update_s_nu(self) -> None:\n",
    "        \"\"\"Update the spike selectors s_k for nu_k.\"\"\"\n",
    "        for k in range(self.k):\n",
    "            # Calculate logits with nu_k=0 (spike)\n",
    "            logits_0 = self.X_aux @ self.gamma[k]\n",
    "            # Calculate logits with nu_k=w_k (slab)\n",
    "            logits_1 = logits_0 + self.theta @ self.w_nu[k]\n",
    "            \n",
    "            # Log-posterior probability for s_k=1 (slab)\n",
    "            log_post_1 = np.log(self.pi_nu) + self._log_likelihood_bernoulli(self.Y[:, k], logits_1)\n",
    "            \n",
    "            # Log-posterior probability for s_k=0 (spike)\n",
    "            log_post_0 = np.log(1 - self.pi_nu) + self._log_likelihood_bernoulli(self.Y[:, k], logits_0)\n",
    "            \n",
    "            # Numerically stable calculation of probability\n",
    "            prob_s1 = 1 / (1 + np.exp(log_post_0 - log_post_1))\n",
    "            self.s_nu[k] = self.rng.binomial(1, prob_s1, size=self.d)\n",
    "\n",
    "    def _log_posterior_w_nu_k(self, k: int, w_nu_k: np.ndarray) -> float:\n",
    "        \"\"\"Calculate log posterior for a single w_nu_k vector.\"\"\"\n",
    "        log_prior = norm_dist.logpdf(w_nu_k, 0, np.sqrt(self.sigma_slab_sq)).sum()\n",
    "        \n",
    "        # The likelihood term only exists if the slab is active (s_nu=1)\n",
    "        nu_k = self.s_nu[k] * w_nu_k\n",
    "        logits = self.X_aux @ self.gamma[k] + self.theta @ nu_k\n",
    "        log_lik = self._log_likelihood_bernoulli(self.Y[:, k], logits)\n",
    "        \n",
    "        return log_prior + log_lik\n",
    "\n",
    "    def _update_w_nu(self, step_size: float = 0.05) -> None:\n",
    "        \"\"\"Update the slab values w_k for nu_k using Metropolis-Hastings.\"\"\"\n",
    "        for k in range(self.k):\n",
    "            current_w_k = self.w_nu[k]\n",
    "            proposal_w_k = self.rng.normal(current_w_k, step_size)\n",
    "            \n",
    "            log_p_curr = self._log_posterior_w_nu_k(k, current_w_k)\n",
    "            log_p_prop = self._log_posterior_w_nu_k(k, proposal_w_k)\n",
    "            \n",
    "            if self.rng.random() < np.exp(log_p_prop - log_p_curr):\n",
    "                self.w_nu[k] = proposal_w_k\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    def step(self) -> None:\n",
    "        \"\"\"Run a single full Gibbs iteration.\"\"\"\n",
    "        # 1. Update latent counts for Poisson part\n",
    "        self._update_latent_counts_z()\n",
    "        \n",
    "        # 2. Update conjugate parameters\n",
    "        self._update_beta()\n",
    "        self._update_eta()\n",
    "        self._update_xi()\n",
    "        \n",
    "        # 3. Update non-conjugate parameters via M-H\n",
    "        self._update_theta()\n",
    "        self._update_gamma()\n",
    "\n",
    "        # 4. Update Spike-and-Slab parameters\n",
    "        self._update_s_nu()\n",
    "        self._update_w_nu()\n",
    "        self.nu = self.s_nu * self.w_nu\n",
    "\n",
    "    def run(self, n_iter: int, n_burnin: int = 100) -> dict:\n",
    "        \"\"\"Run the sampler and return parameter traces.\"\"\"\n",
    "        # Burn-in phase\n",
    "        print(f\"Running burn-in for {n_burnin} iterations...\")\n",
    "        for _ in range(n_burnin):\n",
    "            self.step()\n",
    "\n",
    "        # Sampling phase\n",
    "        print(f\"Running sampling for {n_iter} iterations...\")\n",
    "        traces = {\n",
    "            \"theta\": np.zeros((n_iter, *self.theta.shape)),\n",
    "            \"beta\": np.zeros((n_iter, *self.beta.shape)),\n",
    "            \"gamma\": np.zeros((n_iter, *self.gamma.shape)),\n",
    "            \"nu\": np.zeros((n_iter, *self.nu.shape)),\n",
    "            \"s_nu\": np.zeros((n_iter, *self.s_nu.shape)),\n",
    "        }\n",
    "\n",
    "        for t in range(n_iter):\n",
    "            self.step()\n",
    "            traces[\"theta\"][t] = self.theta\n",
    "            traces[\"beta\"][t] = self.beta\n",
    "            traces[\"gamma\"][t] = self.gamma\n",
    "            traces[\"nu\"][t] = self.nu\n",
    "            traces[\"s_nu\"][t] = self.s_nu\n",
    "            if (t + 1) % 100 == 0:\n",
    "                print(f\"  ...iteration {t+1}/{n_iter}\")\n",
    "\n",
    "        return traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import Generator, default_rng\n",
    "from scipy.special import expit, gammaln, logsumexp, log_expit\n",
    "from scipy.stats import gamma as gamma_dist\n",
    "from scipy.stats import norm as norm_dist\n",
    "\n",
    "class LogSpaceBayesianHybridModelSampler:\n",
    "    \"\"\"\n",
    "    Gibbs sampler for a hybrid model, implemented for numerical stability in log-space.\n",
    "\n",
    "    This implementation prioritizes numerical stability by performing key calculations\n",
    "    in the log domain to prevent underflow from multiplying small probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        Y: np.ndarray,\n",
    "        X_aux: np.ndarray,\n",
    "        n_latent_dims: int,\n",
    "        *,\n",
    "        alpha_theta: float = 0.3,\n",
    "        alpha_beta: float = 0.3,\n",
    "        alpha_xi: float = 0.3,\n",
    "        alpha_eta: float = 0.3,\n",
    "        lambda_xi: float = 0.3,\n",
    "        lambda_eta: float = 0.3,\n",
    "        pi_nu: float = 0.5,\n",
    "        sigma_slab_sq: float = 1.0,\n",
    "        sigma_gamma_sq: float = 1.0,\n",
    "        seed: int | None = None,\n",
    "    ) -> None:\n",
    "        self.rng: Generator = default_rng(seed)\n",
    "        self.X = np.asarray(X, dtype=np.int32)\n",
    "        self.Y = np.asarray(Y, dtype=np.int32)\n",
    "        self.X_aux = np.asarray(X_aux)\n",
    "        self.n, self.p = self.X.shape\n",
    "        self.k = self.Y.shape[1]\n",
    "        self.d = n_latent_dims\n",
    "        self.q = self.X_aux.shape[1]\n",
    "        self.alpha_theta = alpha_theta\n",
    "        self.alpha_beta = alpha_beta\n",
    "        self.alpha_xi = alpha_xi\n",
    "        self.alpha_eta = alpha_eta\n",
    "        self.lambda_xi = lambda_xi\n",
    "        self.lambda_eta = lambda_eta\n",
    "        self.pi_nu = pi_nu\n",
    "        self.sigma_slab_sq = sigma_slab_sq\n",
    "        self.sigma_gamma_sq = sigma_gamma_sq\n",
    "        self._init_params()\n",
    "\n",
    "    # def _init_params(self) -> None:\n",
    "    #     \"\"\"Initialise latent variables and parameters.\"\"\"\n",
    "    #     self.theta = self.rng.gamma(1.0, 1.0, size=(self.n, self.d))\n",
    "    #     self.beta = self.rng.gamma(1.0, 1.0, size=(self.p, self.d))\n",
    "    #     self.xi = self.rng.gamma(self.alpha_xi, scale=1.0/self.lambda_xi, size=self.n)\n",
    "    #     self.eta = self.rng.gamma(self.alpha_eta, scale=1.0/self.lambda_eta, size=self.p)\n",
    "    #     self.z = np.zeros((self.n, self.p, self.d), dtype=np.int32)\n",
    "    #     self._update_latent_counts_z() # Initial population of z\n",
    "    #     self.gamma = self.rng.normal(0.0, np.sqrt(self.sigma_gamma_sq), size=(self.k, self.q))\n",
    "    #     self.s_nu = self.rng.binomial(1, self.pi_nu, size=(self.k, self.d))\n",
    "    #     self.w_nu = self.rng.normal(0.0, np.sqrt(self.sigma_slab_sq), size=(self.k, self.d))\n",
    "    #     self.nu = self.s_nu * self.w_nu\n",
    "\n",
    "        def _init_params(self) -> None:\n",
    "            \"\"\"\n",
    "            Initialise latent variables and parameters by sampling from their priors,\n",
    "            following the model's generative process.\n",
    "            \"\"\"\n",
    "            # 1. Initialize the top-level parent variables first.\n",
    "            #    xi ~ Gamma(alpha_xi, lambda_xi)\n",
    "            #    eta ~ Gamma(alpha_eta, lambda_eta)\n",
    "            self.xi = self.rng.gamma(self.alpha_xi, scale=1.0 / self.lambda_xi, size=self.n)\n",
    "            self.eta = self.rng.gamma(self.alpha_eta, scale=1.0 / self.lambda_eta, size=self.p)\n",
    "\n",
    "            # 2. Now initialize the children using the sampled parents as their parameters.\n",
    "            #    theta_i ~ Gamma(alpha_theta, xi_i)\n",
    "            #    beta_j ~ Gamma(alpha_beta, eta_j)\n",
    "            #    Note: We sample each row with its corresponding rate from xi/eta.\n",
    "            self.theta = np.zeros((self.n, self.d))\n",
    "            for i in range(self.n):\n",
    "                self.theta[i, :] = self.rng.gamma(self.alpha_theta, scale=1.0 / self.xi[i], size=self.d)\n",
    "\n",
    "            self.beta = np.zeros((self.p, self.d))\n",
    "            for j in range(self.p):\n",
    "                self.beta[j, :] = self.rng.gamma(self.alpha_beta, scale=1.0 / self.eta[j], size=self.d)\n",
    "\n",
    "            # 3. Initialize the logistic regression parameters from their priors.\n",
    "            #    gamma_k ~ Normal(0, sigma_gamma_sq)\n",
    "            self.gamma = self.rng.normal(0.0, np.sqrt(self.sigma_gamma_sq), size=(self.k, self.q))\n",
    "\n",
    "            #    Initialize spike-and-slab components from their priors.\n",
    "            #    s_nu_k ~ Bernoulli(pi_nu)\n",
    "            #    w_nu_k ~ Normal(0, sigma_slab_sq)\n",
    "            self.s_nu = self.rng.binomial(1, self.pi_nu, size=(self.k, self.d))\n",
    "            self.w_nu = self.rng.normal(0.0, np.sqrt(self.sigma_slab_sq), size=(self.k, self.d))\n",
    "            #    Combine them to get the initial nu (upsilon).\n",
    "            self.nu = self.s_nu * self.w_nu\n",
    "\n",
    "            # 4. Finally, populate the latent counts `z` based on the initial `theta` and `beta`.\n",
    "            self.z = np.zeros((self.n, self.p, self.d), dtype=np.int32)\n",
    "            self._update_latent_counts_z()\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # --- Log-Space and Conjugate Updates ---\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    def _update_latent_counts_z(self) -> None:\n",
    "        \"\"\"Sample latent counts z_ijl in log-space for numerical stability.\"\"\"\n",
    "        with np.errstate(divide='ignore'): # Ignore log(0) warnings\n",
    "            log_theta = np.log(self.theta)\n",
    "            log_beta = np.log(self.beta)\n",
    "\n",
    "        # Calculate log-rates for each latent dimension component\n",
    "        log_rates = log_theta[:, np.newaxis, :] + log_beta[np.newaxis, :, :] # Shape (n, p, d)\n",
    "        \n",
    "        # Normalize in log-space using logsumexp\n",
    "        log_total_rates = logsumexp(log_rates, axis=2)\n",
    "        log_probs = log_rates - log_total_rates[..., np.newaxis]\n",
    "\n",
    "        # Convert back to probabilities only for the final sampling step\n",
    "        probs = np.exp(log_probs)\n",
    "\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.p):\n",
    "                if self.X[i, j] > 0:\n",
    "                    # Renormalize to handle any minor floating point inaccuracies\n",
    "                    pvals = probs[i, j, :] / np.sum(probs[i, j, :])\n",
    "                    self.z[i, j, :] = self.rng.multinomial(n=self.X[i, j], pvals=pvals)\n",
    "                else:\n",
    "                    self.z[i, j, :] = 0\n",
    "\n",
    "    def _update_beta(self) -> None:\n",
    "        \"\"\"Update beta_jl. This calculation does not require log-space.\"\"\"\n",
    "        z_sum_over_i = np.sum(self.z, axis=0)\n",
    "        shape = self.alpha_beta + z_sum_over_i\n",
    "        rate = self.eta[:, np.newaxis] + np.sum(self.theta, axis=0)\n",
    "        self.beta = self.rng.gamma(shape, scale=1.0 / rate)\n",
    "\n",
    "    def _update_eta(self) -> None:\n",
    "        \"\"\"Update eta_j. This calculation does not require log-space.\"\"\"\n",
    "        shape = self.alpha_eta + self.d * self.alpha_beta\n",
    "        rate = self.lambda_eta + np.sum(self.beta, axis=1)\n",
    "        self.eta = self.rng.gamma(shape, scale=1.0 / rate)\n",
    "\n",
    "    def _update_xi(self) -> None:\n",
    "        \"\"\"Update xi_i. This calculation does not require log-space.\"\"\"\n",
    "        shape = self.alpha_xi + self.d * self.alpha_theta\n",
    "        rate = self.lambda_xi + np.sum(self.theta, axis=1)\n",
    "        self.xi = self.rng.gamma(shape, scale=1.0 / rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def _log_likelihood_bernoulli(y: np.ndarray, logits: np.ndarray) -> float:\n",
    "        \"\"\"Numerically stable Bernoulli log-likelihood using log_expit.\"\"\"\n",
    "        log_p1 = log_expit(logits)  # log(sigmoid(logits))\n",
    "        log_p0 = log_expit(-logits) # log(1 - sigmoid(logits))\n",
    "        return np.sum(y * log_p1 + (1 - y) * log_p0)\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    # --- Non-Conjugate Metropolis-Hastings Updates (already in log-space) ---\n",
    "    # ------------------------------------------------------------------\n",
    "\n",
    "    # def _log_posterior_theta_i(self, i: int, theta_i: np.ndarray) -> float:\n",
    "    #     \"\"\"Calculate log posterior for a single theta_i vector.\"\"\"\n",
    "    #     log_prior_gamma = np.sum(gamma_dist.logpdf(theta_i, a=self.alpha_theta, scale=1.0/self.xi[i]))\n",
    "    #     with np.errstate(divide='ignore'):\n",
    "    #         log_lik_poisson = np.sum(self.z[i, :, :] * np.log(self.beta) - self.beta * theta_i[:, np.newaxis].T)\n",
    "    #     logits = self.X_aux[i] @ self.gamma.T + theta_i @ self.nu.T\n",
    "    #     log_lik_logistic = self._log_likelihood_bernoulli(self.Y[i, :], logits)\n",
    "    #     return log_prior_gamma + log_lik_poisson + log_lik_logistic\n",
    "\n",
    "    # def _update_theta(self, step_size: float = 0.05) -> None:\n",
    "    #     \"\"\"Update theta using Metropolis-Hastings. Calculation is in log-space.\"\"\"\n",
    "    #     for i in range(self.n):\n",
    "    #         current_theta_i = self.theta[i]\n",
    "    #         proposal_theta_i = self.rng.lognormal(np.log(current_theta_i), step_size)\n",
    "    #         log_p_curr = self._log_posterior_theta_i(i, current_theta_i) + np.sum(np.log(current_theta_i))\n",
    "    #         log_p_prop = self._log_posterior_theta_i(i, proposal_theta_i) + np.sum(np.log(proposal_theta_i))\n",
    "    #         if self.rng.random() < np.exp(log_p_prop - log_p_curr):\n",
    "    #             self.theta[i] = proposal_theta_i\n",
    "\n",
    "    # def _log_posterior_gamma_k(self, k: int, gamma_k: np.ndarray) -> float:\n",
    "    #     \"\"\"Calculate log posterior for a single gamma_k vector.\"\"\"\n",
    "    #     log_prior = norm_dist.logpdf(gamma_k, 0, np.sqrt(self.sigma_gamma_sq)).sum()\n",
    "    #     logits = self.X_aux @ gamma_k + self.theta @ self.nu[k]\n",
    "    #     log_lik = self._log_likelihood_bernoulli(self.Y[:, k], logits)\n",
    "    #     return log_prior + log_lik\n",
    "\n",
    "    # def _update_gamma(self, step_size: float = 0.05) -> None:\n",
    "    #     \"\"\"Update gamma using Metropolis-Hastings. Calculation is in log-space.\"\"\"\n",
    "    #     for k in range(self.k):\n",
    "    #         current_gamma_k = self.gamma[k]\n",
    "    #         proposal_gamma_k = self.rng.normal(current_gamma_k, step_size)\n",
    "    #         log_p_curr = self._log_posterior_gamma_k(k, current_gamma_k)\n",
    "    #         log_p_prop = self._log_posterior_gamma_k(k, proposal_gamma_k)\n",
    "    #         if self.rng.random() < np.exp(log_p_prop - log_p_curr):\n",
    "    #             self.gamma[k] = proposal_gamma_k\n",
    "\n",
    "    # # ------------------------------------------------------------------\n",
    "    # # --- Spike-and-Slab Updates for nu (upsilon) (already in log-space) ---\n",
    "    # # ------------------------------------------------------------------\n",
    "\n",
    "    # def _update_s_nu(self) -> None:\n",
    "    #     \"\"\"Update the spike selectors s_k. Calculation is in log-space.\"\"\"\n",
    "    #     for k in range(self.k):\n",
    "    #         logits_0 = self.X_aux @ self.gamma[k] # Logits when nu_k=0\n",
    "    #         logits_1 = logits_0 + self.theta @ self.w_nu[k] # Logits when nu_k=w_k\n",
    "    #         log_post_1 = np.log(self.pi_nu) + self._log_likelihood_bernoulli(self.Y[:, k], logits_1)\n",
    "    #         log_post_0 = np.log(1 - self.pi_nu) + self._log_likelihood_bernoulli(self.Y[:, k], logits_0)\n",
    "            \n",
    "    #         # Stable calculation of P(s=1) = sigmoid(log_post_1 - log_post_0)\n",
    "    #         prob_s1 = expit(log_post_1 - log_post_0)\n",
    "    #         self.s_nu[k] = self.rng.binomial(1, prob_s1, size=self.d)\n",
    "\n",
    "    # def _log_posterior_w_nu_k(self, k: int, w_nu_k: np.ndarray) -> float:\n",
    "    #     \"\"\"Calculate log posterior for a single w_nu_k vector.\"\"\"\n",
    "    #     log_prior = norm_dist.logpdf(w_nu_k, 0, np.sqrt(self.sigma_slab_sq)).sum()\n",
    "    #     nu_k = self.s_nu[k] * w_nu_k\n",
    "    #     logits = self.X_aux @ self.gamma[k] + self.theta @ nu_k\n",
    "    #     log_lik = self._log_likelihood_bernoulli(self.Y[:, k], logits)\n",
    "    #     return log_prior + log_lik\n",
    "\n",
    "    # def _update_w_nu(self, step_size: float = 0.05) -> None:\n",
    "    #     \"\"\"Update the slab values w_k. Calculation is in log-space.\"\"\"\n",
    "    #     for k in range(self.k):\n",
    "    #         current_w_k = self.w_nu[k]\n",
    "    #         proposal_w_k = self.rng.normal(current_w_k, step_size)\n",
    "    #         log_p_curr = self._log_posterior_w_nu_k(k, current_w_k)\n",
    "    #         log_p_prop = self._log_posterior_w_nu_k(k, proposal_w_k)\n",
    "    #         if self.rng.random() < np.exp(log_p_prop - log_p_curr):\n",
    "    #             self.w_nu[k] = proposal_w_k\n",
    "\n",
    "    def _update_theta(self) -> None:\n",
    "        \"\"\"Update theta using Laplace Approximation.\"\"\"\n",
    "        for i in range(self.n):\n",
    "            # The function to minimize is the *negative* log posterior\n",
    "            def objective_func(theta_i):\n",
    "                # Ensure positivity during optimization\n",
    "                if np.any(theta_i <= 0):\n",
    "                    return np.inf\n",
    "                return -self._log_posterior_theta_i(i, theta_i)\n",
    "\n",
    "            # Use the current value as the starting point for the optimization\n",
    "            initial_guess = self.theta[i]\n",
    "\n",
    "            # Find the mode of the posterior by minimizing the negative log posterior\n",
    "            result = minimize(\n",
    "                fun=objective_func,\n",
    "                x0=initial_guess,\n",
    "                method='Nelder-Mead', # A gradient-free method, good for stability\n",
    "            )\n",
    "\n",
    "            if result.success:\n",
    "                self.theta[i] = result.x\n",
    "\n",
    "    def _update_gamma(self) -> None:\n",
    "        \"\"\"Update gamma using Laplace Approximation.\"\"\"\n",
    "        for k in range(self.k):\n",
    "            def objective_func(gamma_k):\n",
    "                return -self._log_posterior_gamma_k(k, gamma_k)\n",
    "\n",
    "            initial_guess = self.gamma[k]\n",
    "            result = minimize(fun=objective_func, x0=initial_guess, method='BFGS') # A gradient-based method\n",
    "\n",
    "            if result.success:\n",
    "                self.gamma[k] = result.x\n",
    "    \n",
    "    # --- Spike-and-Slab Updates for nu (upsilon) ---\n",
    "\n",
    "    # _update_s_nu remains the same, as it's already a closed-form Gibbs step.\n",
    "    def _update_s_nu(self) -> None:\n",
    "        # ... (no change here)\n",
    "        for k in range(self.k):\n",
    "            logits_0 = self.X_aux @ self.gamma[k]\n",
    "            logits_1 = logits_0 + self.theta @ self.w_nu[k]\n",
    "            log_post_1 = np.log(self.pi_nu) + self._log_likelihood_bernoulli(self.Y[:, k], logits_1)\n",
    "            log_post_0 = np.log(1 - self.pi_nu) + self._log_likelihood_bernoulli(self.Y[:, k], logits_0)\n",
    "            prob_s1 = expit(log_post_1 - log_post_0)\n",
    "            self.s_nu[k] = self.rng.binomial(1, prob_s1, size=self.d)\n",
    "\n",
    "    def _update_w_nu(self) -> None:\n",
    "        \"\"\"Update the slab values w_k using Laplace Approximation.\"\"\"\n",
    "        for k in range(self.k):\n",
    "            # Only update w_nu_k if at least one of its components is active\n",
    "            if np.any(self.s_nu[k] == 1):\n",
    "                def objective_func(w_nu_k):\n",
    "                    return -self._log_posterior_w_nu_k(k, w_nu_k)\n",
    "\n",
    "                initial_guess = self.w_nu[k]\n",
    "                result = minimize(fun=objective_func, x0=initial_guess, method='BFGS')\n",
    "\n",
    "                if result.success:\n",
    "                    self.w_nu[k] = result.x\n",
    "            else:\n",
    "                # If all s_nu are 0, just sample w_nu from its prior\n",
    "                self.w_nu[k] = self.rng.normal(0.0, np.sqrt(self.sigma_slab_sq), size=self.d)\n",
    "    # ------------------------------------------------------------------\n",
    "    # --- Sampler Execution ---\n",
    "    # ------------------------------------------------------------------\n",
    "    def step(self) -> None:\n",
    "        \"\"\"Run a single full Gibbs iteration.\"\"\"\n",
    "        self._update_latent_counts_z()\n",
    "        self._update_beta()\n",
    "        self._update_eta()\n",
    "        self._update_xi()\n",
    "        self._update_theta()\n",
    "        self._update_gamma()\n",
    "        self._update_s_nu()\n",
    "        self._update_w_nu()\n",
    "        self.nu = self.s_nu * self.w_nu\n",
    "\n",
    "    def run(self, n_iter: int, n_burnin: int = 100) -> dict:\n",
    "        \"\"\"Run the sampler and return parameter traces.\"\"\"\n",
    "        print(f\"Running burn-in for {n_burnin} iterations...\")\n",
    "        for i in range(n_burnin):\n",
    "            self.step()\n",
    "            if (i + 1) % 100 == 0:\n",
    "                print(f\"  ...burn-in iteration {i+1}/{n_burnin}\")\n",
    "\n",
    "        print(f\"Running sampling for {n_iter} iterations...\")\n",
    "        traces = {\n",
    "            \"theta\": np.zeros((n_iter, *self.theta.shape)),\n",
    "            \"beta\": np.zeros((n_iter, *self.beta.shape)),\n",
    "            \"gamma\": np.zeros((n_iter, *self.gamma.shape)),\n",
    "            \"nu\": np.zeros((n_iter, *self.nu.shape)),\n",
    "            \"s_nu\": np.zeros((n_iter, *self.s_nu.shape)),\n",
    "        }\n",
    "        for t in range(n_iter):\n",
    "            self.step()\n",
    "            traces[\"theta\"][t], traces[\"beta\"][t], traces[\"gamma\"][t], traces[\"nu\"][t], traces[\"s_nu\"][t] = \\\n",
    "                self.theta, self.beta, self.gamma, self.nu, self.s_nu\n",
    "            if (t + 1) % 100 == 0:\n",
    "                print(f\"  ...sampling iteration {t+1}/{n_iter}\")\n",
    "        return traces"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
